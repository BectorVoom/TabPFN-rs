# TabPFN-rs Architecture Structure

## Top-Level Project Structure
```
TabPFN-rs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs                 # Main library entry point
â”‚   â”œâ”€â”€ main.rs               # Main application entry
â”‚   â”œâ”€â”€ bin/                  # Test and debug binaries
â”‚   â””â”€â”€ tabpfn/               # Core TabPFN implementation
â”œâ”€â”€ Cargo.toml               # Project configuration
â””â”€â”€ Cargo.lock              # Dependency lock file
```

## Core Module Organization

### Library Structure (`src/lib.rs`)
- Exports the main `tabpfn` module
- Provides access to `Settings` and `settings` function
- Entry point: `pub use tabpfn::settings::{Settings, settings};`

### Main TabPFN Module (`src/tabpfn/`)
```
src/tabpfn/
â”œâ”€â”€ mod.rs                   # Module exports (settings, architectures)
â”œâ”€â”€ settings.rs              # Configuration management
â””â”€â”€ architectures/           # Neural network architectures
```

## Architecture Components (`src/tabpfn/architectures/`)

### Module Structure
```
src/tabpfn/architectures/
â”œâ”€â”€ mod.rs                   # Architecture module exports
â”œâ”€â”€ interface.rs             # Architecture interfaces/traits
â”œâ”€â”€ instance.rs              # Architecture instances
â”œâ”€â”€ simple_instance.rs       # Simplified architecture instances
â””â”€â”€ base/                    # Core neural network components
```

### Base Components (`src/tabpfn/architectures/base/`)
```
src/tabpfn/architectures/base/
â”œâ”€â”€ mod.rs                   # Base module exports
â”œâ”€â”€ config.rs                # Configuration structures
â”œâ”€â”€ encoders.rs              # Data encoding components
â”œâ”€â”€ layer.rs                 # Neural network layers
â”œâ”€â”€ memory.rs                # Memory management utilities
â”œâ”€â”€ mlp.rs                   # Multi-Layer Perceptron implementation
â”œâ”€â”€ transformer.rs           # Transformer architecture
â””â”€â”€ attention/               # Attention mechanisms
    â”œâ”€â”€ mod.rs
    â””â”€â”€ full_attention.rs    # âœ… Complete multi-head attention
```

## Binary Targets (`src/bin/`)

### Testing Binaries
- `test_attention_equivalence.rs` - Validates attention implementation against Python
- `test_mlp_equivalence.rs` - Validates MLP implementation against Python  
- `test_layer_equivalence.rs` - Validates layer implementation against Python
- `test_dropout_functionality.rs` - Tests dropout behavior
- `test_forward_method_equivalence.rs` - Tests forward pass methods
- `test_cache_update_equivalence.rs` - Tests caching mechanisms
- `test_set_parameters.rs` - Tests parameter setting functionality
- `test_transformer_graph_operations.rs` - Tests transformer graph operations

### Debug Utilities
- `debug_weights.rs` - Weight inspection and debugging tools

## Key Architectural Patterns

### Configuration System
- **Settings Hierarchy**: `Settings` â†’ `TabPFNSettings`, `TestingSettings`, `PytorchSettings`
- **Global Access**: Via `SETTINGS` constant and `settings()` function
- **Serialization**: Full serde support for all configuration structures

### Neural Network Components
- **Backend Abstraction**: Uses Burn framework with configurable backends (NdArray, WGPU, CUDA)
- **Module Pattern**: All components implement Burn's `Module` trait
- **Parameter Management**: Uses `Param<Tensor>` for trainable parameters

### Python Equivalence Structure
- **Direct Mapping**: Rust modules mirror Python file structure
- **Equivalence Testing**: Dedicated test binaries for each major component
- **Documentation Links**: Each Rust file references corresponding Python file

## Implementation Status

### âœ… Completed Components
- **Full Attention** (`full_attention.rs`) - Complete with comprehensive testing
- **MLP** (`mlp.rs`) - Multi-layer perceptron with activation functions
- **Settings System** (`settings.rs`) - Configuration management
- **Core Infrastructure** - Module organization and build system

### ðŸš§ In Development
- Various layer implementations
- Encoder components  
- Memory management utilities
- Transformer integration

## Data Flow Architecture

### Typical Processing Pipeline
1. **Configuration Loading** - Via settings system
2. **Data Preprocessing** - Through encoder components
3. **Neural Network Processing** - MLP, attention, transformer layers
4. **Output Generation** - Via appropriate output layers

### Backend Support
- **NdArray Backend** - CPU computation using ndarray
- **WGPU Backend** - GPU computation via WebGPU
- **CUDA Backend** - NVIDIA GPU acceleration
- **Autodiff Support** - Automatic differentiation for training

## Module Dependencies

### External Dependencies
- `burn` - Core deep learning framework
- `serde` - Configuration serialization
- `polars` - Data manipulation
- `tokio` - Async runtime support

### Internal Dependencies
- Settings system used throughout all modules
- Base components shared across architecture implementations
- Test utilities shared across equivalence testing binaries