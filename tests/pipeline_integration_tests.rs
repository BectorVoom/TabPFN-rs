use burn::{
    backend::{ndarray::NdArrayDevice, Autodiff, NdArray},
    tensor::{Tensor, TensorData},
};
use std::collections::HashMap;
use tab_pfn_rs::tabpfn::architectures::base::{
    train::{DatasetPrior, TrainingConfig, PriorType, TabPFNTrainer, argmax_with_tie_break_smallest},
    transformer::DeterministicRngContext,
    config::ModelConfig,
    loss_utils,
};
use rand::{rngs::StdRng, SeedableRng};

type TestBackend = Autodiff<NdArray>;

/// End-to-end pipeline integration tests for canonical tensor shape consistency
/// 
/// These tests verify that the entire TabPFN training pipeline maintains canonical
/// tensor shapes throughout all stages:
/// 1. Dataset generation â†’ [S, B, F], [S, B], [S, B], etc.
/// 2. Model input preparation â†’ proper tensor formatting for transformers
/// 3. Forward pass â†’ [S, B, C] logits output
/// 4. Loss computation â†’ proper reshape to [S*B, C] and [S*B]
/// 5. Argmax operation â†’ [S, B] predictions with deterministic tie-breaking

#[test]
fn test_end_to_end_pipeline_shape_consistency() {
    println!("ðŸ§ª Testing end-to-end pipeline shape consistency");
    
    let device = NdArrayDevice::Cpu;
    let rng_context = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    let mut rng = StdRng::seed_from_u64(123);
    
    // Create realistic test configuration
    let config = TrainingConfig {
        meta_batch_size: 3,
        num_features_range: (5, 8),
        num_classes_range: (3, 5),
        feature_noise_level: 0.1,
        prior_type: PriorType::Gaussian,
        model: ModelConfig::default(),
        learning_rate: 0.001,
        layer_dropout_min_layers: 2,
        cache_trainset_representations: false,
    };
    
    let prior = DatasetPrior::new(&config);
    let seq_len = 20;
    let batch_size = 3;
    
    // STAGE 1: Dataset Generation and Validation
    println!("  Stage 1: Dataset generation and validation");
    let dataset = prior.sample(seq_len, batch_size, &device, &rng_context, &mut rng);
    
    // Verify canonical dataset shapes
    assert_eq!(dataset.features.dims().len(), 3, "Features must be 3D");
    assert_eq!(dataset.targets.dims().len(), 2, "Targets must be 2D");
    assert_eq!(dataset.train_mask.dims().len(), 2, "Train mask must be 2D");
    assert_eq!(dataset.labels_for_model.dims().len(), 2, "Labels for model must be 2D");
    
    let [s, b, f] = dataset.features.dims();
    assert_eq!((s, b), (seq_len, batch_size), "Dataset dimensions must match request");
    assert_eq!(dataset.targets.dims(), [s, b], "Targets shape must match features[0:2]");
    assert_eq!(dataset.train_mask.dims(), [s, b], "Train mask shape must match features[0:2]");
    assert_eq!(dataset.labels_for_model.dims(), [s, b], "Labels for model shape must match features[0:2]");
    
    println!("    âœ“ Dataset shapes: features {:?}, targets {:?}", 
             dataset.features.dims(), dataset.targets.dims());
    
    // STAGE 2: Input Preparation for Model
    println!("  Stage 2: Model input preparation");
    
    let mut x_inputs = HashMap::new();
    x_inputs.insert("main".to_string(), dataset.features.clone());
    
    let y_input_3d = dataset.labels_for_model.clone().float().unsqueeze_dim(2);
    let mut y_inputs = HashMap::new();
    y_inputs.insert("main".to_string(), y_input_3d.clone());
    
    // Verify input shapes for model
    assert_eq!(x_inputs["main"].dims(), [s, b, f], "X input must maintain canonical shape");
    assert_eq!(y_inputs["main"].dims(), [s, b, 1], "Y input must be [S, B, 1] for model");
    
    println!("    âœ“ Model inputs: x_shape {:?}, y_shape {:?}", 
             x_inputs["main"].dims(), y_inputs["main"].dims());
    
    // STAGE 3: Simulate Model Forward Pass (create realistic logits)
    println!("  Stage 3: Simulated model forward pass");
    
    let num_classes = 4; // Simulate 4-class output
    
    // Create realistic logits tensor [S, B, C] with some known patterns
    let mut logits_data = Vec::new();
    for s_idx in 0..s {
        for b_idx in 0..b {
            for c_idx in 0..num_classes {
                // Create some predictable patterns including ties
                let base_value = (s_idx as f32 * 0.1) + (c_idx as f32 * 0.3);
                let tie_pattern = if (s_idx + b_idx) % 4 == 0 && c_idx < 2 {
                    // Create 2-way ties at some positions
                    5.0
                } else {
                    base_value + (rng.next_u32() as f32 / u32::MAX as f32) * 0.5
                };
                logits_data.push(tie_pattern);
            }
        }
    }
    
    let model_output = Tensor::<TestBackend, 1>::from_floats(
        logits_data.as_slice(), &device
    ).reshape([s, b, num_classes]);
    
    // Verify model output shape
    assert_eq!(model_output.dims().len(), 3, "Model output must be 3D [S, B, C]");
    assert_eq!(model_output.dims(), [s, b, num_classes], "Model output must be canonical [S, B, C]");
    
    println!("    âœ“ Model output: shape {:?}", model_output.dims());
    
    // STAGE 4: Loss Computation Pipeline
    println!("  Stage 4: Loss computation with shape validation");
    
    // Critical reshape for loss: [S, B, C] â†’ [S*B, C], [S, B] â†’ [S*B]
    let flattened_size = s * b;
    let logits_for_loss = model_output.clone().reshape([flattened_size, num_classes]);
    let labels_for_loss = dataset.labels_for_model.clone().reshape([flattened_size]);
    
    // Verify loss input shapes
    assert_eq!(logits_for_loss.dims(), [flattened_size, num_classes], 
               "Logits for loss must be [S*B, C]");
    assert_eq!(labels_for_loss.dims(), [flattened_size], 
               "Labels for loss must be [S*B]");
    
    // Compute loss using the actual loss function
    let loss = loss_utils::compute_masked_cross_entropy_loss_ignore_index(
        logits_for_loss.clone(),
        labels_for_loss.clone(),
        &device
    );
    
    // Verify loss properties
    let loss_value = loss.clone().into_scalar();
    assert!(loss_value.is_finite(), "Loss must be finite");
    assert!(!loss_value.is_nan(), "Loss must not be NaN");
    assert!(!loss_value.is_infinite(), "Loss must not be infinite");
    assert!(loss_value >= 0.0, "Cross-entropy loss must be non-negative");
    
    println!("    âœ“ Loss computation: value = {:.4} (finite and valid)", loss_value);
    
    // STAGE 5: Argmax with Tie-Breaking
    println!("  Stage 5: Argmax with deterministic tie-breaking");
    
    // Apply canonical argmax: [S, B, C] â†’ [S, B]
    let predictions = argmax_with_tie_break_smallest(model_output.clone());
    
    // Verify prediction shape and values
    assert_eq!(predictions.dims(), [s, b], "Predictions must be [S, B]");
    
    let pred_data = predictions.to_data();
    let pred_slice = pred_data.as_slice::<i64>().unwrap();
    
    for (i, &pred_class) in pred_slice.iter().enumerate() {
        assert!(pred_class >= 0 && pred_class < num_classes as i64,
                "Position {}: prediction {} outside valid range [0, {})", 
                i, pred_class, num_classes);
    }
    
    // Verify tie-breaking behavior for known tie positions
    let ties_found = pred_slice.iter()
        .enumerate()
        .filter(|(i, &pred)| {
            let s_idx = i / b;
            let b_idx = i % b;
            (s_idx + b_idx) % 4 == 0 && pred == 0  // Should pick class 0 for ties
        })
        .count();
    
    println!("    âœ“ Argmax: shape {:?}, found {} tie-breaking cases", 
             predictions.dims(), ties_found);
    
    // STAGE 6: Comprehensive Shape Consistency Verification
    println!("  Stage 6: Final shape consistency verification");
    
    // Verify dimensional relationships across entire pipeline
    assert_eq!(dataset.features.dims()[0], model_output.dims()[0], 
               "Sequence dimension must be consistent: dataset â†’ model");
    assert_eq!(dataset.features.dims()[1], model_output.dims()[1], 
               "Batch dimension must be consistent: dataset â†’ model");
    assert_eq!(model_output.dims()[0], predictions.dims()[0], 
               "Sequence dimension must be consistent: model â†’ predictions");
    assert_eq!(model_output.dims()[1], predictions.dims()[1], 
               "Batch dimension must be consistent: model â†’ predictions");
    assert_eq!(logits_for_loss.dims()[0], labels_for_loss.dims()[0], 
               "Flattened dimension must be consistent: logits â†” labels");
    assert_eq!(logits_for_loss.dims()[0], s * b, 
               "Flattened dimension must equal S*B");
    
    println!("    âœ“ All dimensional relationships verified");
    
    // STAGE 7: Data Consistency Checks
    println!("  Stage 7: Data consistency validation");
    
    // Verify labels_for_model construction
    let targets_data = dataset.targets.to_data();
    let mask_data = dataset.train_mask.to_data();
    let labels_data = dataset.labels_for_model.to_data();
    
    let targets_slice = targets_data.as_slice::<i64>().unwrap();
    let mask_slice = mask_data.as_slice::<bool>().unwrap();
    let labels_slice = labels_data.as_slice::<i64>().unwrap();
    
    let mut correct_label_construction = 0;
    let total_positions = targets_slice.len();
    
    for i in 0..total_positions {
        let expected_label = if mask_slice[i] {
            targets_slice[i]  // Training position: use original target
        } else {
            -1  // Test position: use sentinel value
        };
        if labels_slice[i] == expected_label {
            correct_label_construction += 1;
        }
    }
    
    assert_eq!(correct_label_construction, total_positions,
               "All labels_for_model values must follow construction rule");
    
    println!("    âœ“ Data consistency: {}/{} positions validated", 
             correct_label_construction, total_positions);
    
    println!("âœ… End-to-end pipeline shape consistency test PASSED");
    println!("   Pipeline verified: Dataset â†’ Model â†’ Loss â†’ Predictions");
    println!("   All shapes canonical and dimensionally consistent");
}

#[test]
fn test_multi_batch_pipeline_consistency() {
    println!("ðŸ§ª Testing multi-batch pipeline consistency");
    
    let device = NdArrayDevice::Cpu;
    let rng_context = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    let mut rng = StdRng::seed_from_u64(456);
    
    let config = TrainingConfig {
        meta_batch_size: 8, // Larger batch size
        num_features_range: (10, 15),
        num_classes_range: (3, 6),
        feature_noise_level: 0.2,
        prior_type: PriorType::Gaussian,
        model: ModelConfig::default(),
        learning_rate: 0.001,
        layer_dropout_min_layers: 2,
        cache_trainset_representations: false,
    };
    
    let prior = DatasetPrior::new(&config);
    
    // Test different sequence lengths with same batch size
    let test_cases = vec![
        (50, 8, "standard case"),
        (10, 8, "short sequences"),
        (200, 8, "long sequences"),
    ];
    
    for (seq_len, batch_size, description) in test_cases {
        println!("  Testing {}: S={}, B={}", description, seq_len, batch_size);
        
        let dataset = prior.sample(seq_len, batch_size, &device, &rng_context, &mut rng);
        
        // Verify all tasks have proper train/test distribution
        for task_idx in 0..batch_size {
            let task_mask = dataset.train_mask.clone().select(1, task_idx);
            let has_train = task_mask.clone().any().into_scalar();
            let has_test = task_mask.clone().bool_not().any().into_scalar();
            
            assert!(has_train && has_test, 
                   "{}: Task {} missing train or test samples", description, task_idx);
        }
        
        // Simulate forward pass and verify shapes
        let num_classes = 4;
        let mock_logits_data: Vec<f32> = (0..seq_len * batch_size * num_classes)
            .map(|i| (i as f32) * 0.01)
            .collect();
        
        let mock_logits = Tensor::<TestBackend, 1>::from_floats(
            mock_logits_data.as_slice(), &device
        ).reshape([seq_len, batch_size, num_classes]);
        
        // Test argmax consistency
        let predictions = argmax_with_tie_break_smallest(mock_logits.clone());
        assert_eq!(predictions.dims(), [seq_len, batch_size],
                   "{}: Prediction shape must be [S, B]", description);
        
        // Test loss computation shapes  
        let flattened_size = seq_len * batch_size;
        let logits_flat = mock_logits.reshape([flattened_size, num_classes]);
        let labels_flat = dataset.labels_for_model.reshape([flattened_size]);
        
        assert_eq!(logits_flat.dims(), [flattened_size, num_classes],
                   "{}: Flattened logits shape incorrect", description);
        assert_eq!(labels_flat.dims(), [flattened_size],
                   "{}: Flattened labels shape incorrect", description);
        
        println!("    âœ“ {} pipeline consistency verified", description);
    }
    
    println!("âœ… Multi-batch pipeline consistency test PASSED");
}

#[test]
fn test_pipeline_error_conditions() {
    println!("ðŸ§ª Testing pipeline error condition handling");
    
    let device = NdArrayDevice::Cpu;
    let rng_context = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    let mut rng = StdRng::seed_from_u64(789);
    
    let config = TrainingConfig {
        meta_batch_size: 2,
        num_features_range: (3, 5),
        num_classes_range: (2, 3),
        feature_noise_level: 0.1,
        prior_type: PriorType::Gaussian,
        model: ModelConfig::default(),
        learning_rate: 0.001,
        layer_dropout_min_layers: 2,
        cache_trainset_representations: false,
    };
    
    let prior = DatasetPrior::new(&config);
    let dataset = prior.sample(10, 2, &device, &rng_context, &mut rng);
    
    // Test 1: Mismatched logits shape for loss computation
    println!("  Test 1: Mismatched tensor dimensions");
    
    // Create logits with wrong batch dimension
    let wrong_logits = Tensor::<TestBackend, 1>::from_floats(
        vec![1.0; 10 * 3 * 3].as_slice(), &device  // S=10, B=3 (should be 2), C=3
    ).reshape([10, 3, 3]);
    
    // This should work for argmax (doesn't check batch consistency)
    let wrong_predictions = argmax_with_tie_break_smallest(wrong_logits.clone());
    assert_eq!(wrong_predictions.dims(), [10, 3], "Argmax should still work with wrong batch size");
    
    // Test 2: Valid loss computation with correct dimensions
    println!("  Test 2: Correct loss computation");
    
    let correct_logits = Tensor::<TestBackend, 1>::from_floats(
        vec![1.0; 10 * 2 * 3].as_slice(), &device  // S=10, B=2, C=3  
    ).reshape([10, 2, 3]);
    
    let logits_for_loss = correct_logits.reshape([20, 3]); // 10*2=20
    let labels_for_loss = dataset.labels_for_model.reshape([20]);
    
    // This should work without errors
    let loss = loss_utils::compute_masked_cross_entropy_loss_ignore_index(
        logits_for_loss,
        labels_for_loss,
        &device
    );
    
    let loss_value = loss.into_scalar();
    assert!(loss_value.is_finite(), "Loss should be finite for valid inputs");
    
    println!("    âœ“ Valid loss computation: {:.4}", loss_value);
    
    // Test 3: Edge case with minimal dimensions
    println!("  Test 3: Minimal dimension edge cases");
    
    let minimal_dataset = prior.sample(2, 1, &device, &rng_context, &mut rng); // Minimal viable
    let minimal_logits = Tensor::<TestBackend, 1>::from_floats(
        vec![2.0, 1.0, 3.0, 1.0].as_slice(), &device  // S=2, B=1, C=2
    ).reshape([2, 1, 2]);
    
    let minimal_predictions = argmax_with_tie_break_smallest(minimal_logits.clone());
    assert_eq!(minimal_predictions.dims(), [2, 1], "Minimal predictions shape");
    
    let minimal_logits_flat = minimal_logits.reshape([2, 2]); // 2*1=2
    let minimal_labels_flat = minimal_dataset.labels_for_model.reshape([2]);
    
    let minimal_loss = loss_utils::compute_masked_cross_entropy_loss_ignore_index(
        minimal_logits_flat,
        minimal_labels_flat,
        &device
    );
    
    assert!(minimal_loss.into_scalar().is_finite(), "Minimal loss should be finite");
    
    println!("    âœ“ Minimal dimension edge cases handled");
    
    println!("âœ… Pipeline error condition handling test PASSED");
}

#[test]
fn test_deterministic_pipeline_reproducibility() {
    println!("ðŸ§ª Testing deterministic pipeline reproducibility");
    
    let device = NdArrayDevice::Cpu;
    
    let config = TrainingConfig {
        meta_batch_size: 4,
        tasks_per_batch: 1,
        max_samples_per_task: 100,
        min_samples_per_task: 10,
        num_features_range: (6, 8),
        num_classes_range: (3, 4),
        feature_noise_level: 0.1,
        prior_type: PriorType::Gaussian,
        model: ModelConfig::default(),
        learning_rate: 0.001,
        warmup_steps: 0,
        gradient_accumulation_steps: 1,
        gradient_clip_norm: None,
        num_epochs: 1,
        checkpoint_frequency: 100,
        validation_frequency: 50,
        early_stopping_patience: 10,
        use_gradient_checkpointing: false,
        layer_dropout_min_layers: Some(2),
        cache_trainset_representations: false,
    };
    
    let prior = DatasetPrior::new(&config);
    
    // Run identical pipeline multiple times with same seeds
    let mut pipeline_results = Vec::new();
    
    for run_idx in 0..3 {
        println!("  Run {}: Testing reproducibility", run_idx);
        
        // Use same seeds for reproducibility
        let rng_context = DeterministicRngContext::<TestBackend>::new(12345, device.clone());
        let mut rng = StdRng::seed_from_u64(67890);
        
        let dataset = prior.sample(15, 4, &device, &rng_context, &mut rng);
        
        // Create deterministic logits (same pattern each run)
        let logits_pattern: Vec<f32> = (0..15 * 4 * 4)
            .map(|i| ((i as f32) * 0.03).sin() * 2.0)
            .collect();
        
        let logits = Tensor::<TestBackend, 1>::from_floats(
            logits_pattern.as_slice(), &device
        ).reshape([15, 4, 4]);
        
        let predictions = argmax_with_tie_break_smallest(logits.clone());
        
        // Collect results for comparison
        let pred_data = predictions.to_data();
        let pred_vec = pred_data.as_slice::<i64>().unwrap().to_vec();
        
        let features_data = dataset.features.to_data();
        let features_vec = features_data.as_slice::<f32>().unwrap().to_vec();
        
        pipeline_results.push((pred_vec, features_vec));
        
        println!("    Run {}: {} predictions, {} features", 
                run_idx, pipeline_results[run_idx].0.len(), pipeline_results[run_idx].1.len());
    }
    
    // Verify all runs produced identical results
    for run_idx in 1..pipeline_results.len() {
        assert_eq!(pipeline_results[run_idx].0, pipeline_results[0].0,
                   "Run {} predictions differ from run 0", run_idx);
        
        // Note: Features may differ due to RNG usage in dataset generation
        // This is expected since we're testing pipeline determinism, not dataset determinism
        println!("    âœ“ Run {} predictions match baseline", run_idx);
    }
    
    println!("âœ… Deterministic pipeline reproducibility test PASSED");
    println!("   All runs produced identical argmax results with same RNG seeds");
}