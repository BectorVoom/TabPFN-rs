//! Comprehensive TDD Test Suite for TabPFN Training Components
//! 
//! This test suite implements all requirements from the TDD specification:
//! 1. Argmax/prediction extraction with device/dtype-safe operations and tie-breaking
//! 2. Loss numerical stability for extreme logits  
//! 3. Gradient accumulation equivalence testing
//! 4. Gradient clipping validation
//! 5. RNG reproducibility verification
//!
//! All tests are designed to FAIL initially, following strict TDD methodology.

use burn::{
    backend::Autodiff, 
    tensor::{Tensor, TensorData, Int, backend::{Backend, AutodiffBackend}},
};
use burn_ndarray::NdArray;
use rand::{rngs::StdRng, SeedableRng};
use tab_pfn_rs::tabpfn::{
    architectures::base::{
        train::{TrainingConfig, TabPFNTrainer, PriorType, argmax_with_tie_break_smallest},
        transformer::{DeterministicRngContext, PerFeatureTransformer},
        config::ModelConfig,
        loss_utils,
    },
};

type TestBackend = Autodiff<NdArray<f32>>;

/// Test 1: Device/dtype-safe argmax with deterministic tie-breaking
/// 
/// Requirements:
/// - No use of to_data() or as_slice() on device tensors
/// - Deterministic tie-breaking (smallest index wins)
/// - Works on GPU/CPU agnostic
/// - Handles extreme values correctly
#[test]
fn test_device_safe_argmax_with_tie_breaking() {
    println!("ðŸ”´ Test 1: Device/dtype-safe argmax - EXPECTED TO FAIL");
    
    let device = <TestBackend as Backend>::Device::default();
    
    // Test case 1: Simple tie-breaking
    let logits = Tensor::<TestBackend, 3>::from_floats([
        // Batch 0, Sample 0: tie between classes 0,1 -> expect 0
        [[2.0, 2.0, 1.0]],
        // Batch 0, Sample 1: tie between classes 1,2 -> expect 1  
        [[1.0, 2.5, 2.5]],
    ], &device);
    
    let result = argmax_with_tie_break_smallest(logits.clone());
    let expected = Tensor::<TestBackend, 2, Int>::from_data(
        TensorData::new(vec![0i64, 1i64], [2, 1]), 
        &device
    );
    
    // This should pass when implementation is device-safe
    assert_eq!(result.to_data().as_slice::<i64>().unwrap(), 
               expected.to_data().as_slice::<i64>().unwrap());
    
    // Test case 2: Extreme values stability  
    let extreme_logits = Tensor::<TestBackend, 3>::from_floats([
        [[f32::MAX, f32::MAX, 0.0]],  // Extreme tie -> expect 0
        [[f32::MIN, 0.0, f32::MIN]],  // Middle value wins -> expect 1
    ], &device);
    
    let extreme_result = argmax_with_tie_break_smallest(extreme_logits);
    let extreme_expected = vec![0i64, 1i64];
    assert_eq!(extreme_result.to_data().as_slice::<i64>().unwrap(), &extreme_expected);
    
    panic!("EXPECTED FAILURE: Current argmax uses forbidden to_data() calls");
}

/// Test 2: Loss numerical stability for extreme logits
/// 
/// Requirements:
/// - Loss must be finite for extreme input values
/// - No NaN or Inf in loss computation 
/// - Proper masking with ignore_index=-1
/// - Numerical stability using log-sum-exp tricks
#[test]
fn test_loss_numerical_stability_extreme_values() {
    println!("ðŸ”´ Test 2: Loss numerical stability - EXPECTED TO FAIL");
    
    let device = <TestBackend as Backend>::Device::default();
    
    // Test case 1: Extreme logits that could cause overflow
    let extreme_logits = Tensor::<TestBackend, 2>::from_floats([
        [f32::MAX, 100.0, 50.0],      // Very large values
        [-f32::MAX, -100.0, -50.0],   // Very small values  
        [0.0, f32::NAN, f32::INFINITY], // Special values
        [1000.0, 1000.1, 999.9],      // Close extreme values
    ], &device);
    
    let labels = Tensor::<TestBackend, 1, Int>::from_data(
        TensorData::new(vec![0i64, 1i64, -1i64, 2i64], [4]), // One masked
        &device
    );
    
    let loss = loss_utils::compute_masked_cross_entropy_loss_ignore_index(
        extreme_logits.clone(), labels, &device
    );
    
    let loss_value: f32 = loss.into_scalar();
    
    // Core requirement: Loss must be finite 
    assert!(loss_value.is_finite(), "Loss must be finite, got: {}", loss_value);
    assert!(!loss_value.is_nan(), "Loss must not be NaN");  
    assert!(!loss_value.is_infinite(), "Loss must not be infinite");
    
    // Test case 2: All masked inputs should give zero loss
    let all_masked_labels = Tensor::<TestBackend, 1, Int>::from_data(
        TensorData::new(vec![-1i64, -1i64, -1i64, -1i64], [4]),
        &device
    );
    
    let masked_loss = loss_utils::compute_masked_cross_entropy_loss_ignore_index(
        extreme_logits.clone(), all_masked_labels, &device
    );
    let masked_loss_value: f32 = masked_loss.into_scalar();
    
    assert_eq!(masked_loss_value, 0.0, "All-masked loss should be 0");
    
    panic!("EXPECTED FAILURE: Current loss implementation may not handle extreme values properly");
}

/// Test 3: Gradient accumulation equivalence
/// 
/// Requirements:
/// - Single large batch == N accumulation steps with batch/N size
/// - Parameters after accumulation must be approximately equal
/// - Tolerance check with params_close function
/// - Optimizer.step() called only after accumulation window
#[test]
fn test_gradient_accumulation_equivalence() {
    println!("ðŸ”´ Test 3: Gradient accumulation equivalence - EXPECTED TO FAIL");
    
    let device = <TestBackend as Backend>::Device::default();
    let rng_context = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    
    // Create minimal config for testing
    let mut config = create_minimal_test_config();
    config.gradient_accumulation_steps = 4;  // Test accumulation over 4 steps
    config.meta_batch_size = 8;              // Total batch size
    
    // Test setup: Create two trainers with same initial state
    let trainer1 = TabPFNTrainer::new(config.clone(), &device, rng_context.clone());
    let trainer2 = TabPFNTrainer::new(config.clone(), &device, rng_context.clone());
    
    // Method 1: Single large batch (reference)
    let mut large_batch_trainer = trainer1;
    let mut large_rng = StdRng::seed_from_u64(42);
    
    // Method 2: Gradient accumulation over 4 steps  
    let mut accumulation_trainer = trainer2;
    let mut accum_rng = StdRng::seed_from_u64(42);
    
    // Execute training - this should use gradient accumulation logic
    let _loss1 = large_batch_trainer.train_step(&device, &mut large_rng);
    let _loss2 = accumulation_trainer.train_step(&device, &mut accum_rng);
    
    // Compare final parameters - they should be approximately equal
    let params1 = large_batch_trainer.model.clone();
    let params2 = accumulation_trainer.model.clone();
    
    // This function should be implemented to compare model parameters
    assert!(params_close(&params1, &params2, 1e-6), 
            "Parameters after gradient accumulation should match single large batch");
    
    panic!("EXPECTED FAILURE: Gradient accumulation not properly implemented");
}

/// Test 4: Gradient clipping validation
/// 
/// Requirements:  
/// - Artificially large gradients get clipped to threshold
/// - Gradient norm <= clip_threshold + small epsilon
/// - Clipping preserves gradient direction
/// - Global gradient norm computation across all parameters
#[test]
fn test_gradient_clipping_validation() {
    println!("ðŸ”´ Test 4: Gradient clipping validation - EXPECTED TO FAIL");
    
    let device = <TestBackend as Backend>::Device::default();
    let rng_context = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    
    let mut config = create_minimal_test_config();
    config.gradient_clip_norm = Some(1.0);  // Set clipping threshold
    
    let mut trainer = TabPFNTrainer::new(config, &device, rng_context);
    let mut rng = StdRng::seed_from_u64(42);
    
    // Create artificially large loss to generate large gradients
    // This should trigger gradient clipping
    let _loss = trainer.train_step(&device, &mut rng);
    
    // After training step, gradients should be clipped
    let clip_threshold = 1.0;
    let grad_norm = compute_global_gradient_norm(&trainer.model);
    
    assert!(grad_norm <= clip_threshold + 1e-8, 
            "Gradient norm {} should be <= clip threshold {} + eps", 
            grad_norm, clip_threshold);
    
    panic!("EXPECTED FAILURE: Gradient clipping not implemented");
}

/// Test 5: RNG reproducibility and centralization
/// 
/// Requirements:
/// - Same seed produces identical results (bitwise or approximate)  
/// - All randomness flows through DeterministicRngContext
/// - Multiple runs with same seed are deterministic
/// - Different seeds produce different results
#[test]
fn test_rng_reproducibility_and_centralization() {
    println!("ðŸ”´ Test 5: RNG reproducibility - EXPECTED TO FAIL");
    
    let device = <TestBackend as Backend>::Device::default();
    
    // Test deterministic reproduction
    let seed = 12345u64;
    let results1 = run_training_with_seed(seed, &device);
    let results2 = run_training_with_seed(seed, &device);
    
    // Results should be identical
    assert_eq!(results1, results2, "Same seed should produce identical results");
    
    // Test different seeds produce different results
    let results3 = run_training_with_seed(54321u64, &device);
    assert_ne!(results1, results3, "Different seeds should produce different results");
    
    // Test multiple iterations with same seed
    let mut consistent_results = Vec::new();
    for _ in 0..5 {
        consistent_results.push(run_training_with_seed(seed, &device));
    }
    
    // All results should be identical
    for result in &consistent_results[1..] {
        assert_eq!(consistent_results[0], *result, 
                  "All runs with same seed should be identical");
    }
    
    panic!("EXPECTED FAILURE: RNG centralization may not be complete");
}

// Helper functions for the tests

fn create_minimal_test_config() -> TrainingConfig {
    TrainingConfig {
        model: ModelConfig {
            max_num_classes: 10,
            num_buckets: 100,
            seed: 42,
            emsize: 16,
            nhid_factor: 2,
            nlayers: 2,
            features_per_group: 2,
            nhead: 2,
            feature_positional_embedding: None,
            use_separate_decoder: true,
            dropout: 0.0,
            encoder_use_bias: false,
            multiquery_item_attention: false,
            nan_handling_enabled: true,
            nan_handling_y_encoder: true,
            normalize_by_used_features: true,
            normalize_on_train_only: true,
            normalize_to_ranking: false,
            normalize_x: true,
            recompute_attn: false,
            recompute_layer: true,
            remove_empty_features: true,
            remove_outliers: false,
            multiquery_item_attention_for_test_set: true,
            attention_init_gain: 1.0,
            dag_pos_enc_dim: None,
            item_attention_type: "full".to_string(),
            feature_attention_type: "full".to_string(),
            remove_duplicate_features: false,
        },
        meta_batch_size: 2,
        tasks_per_batch: 4,
        max_samples_per_task: 8,
        min_samples_per_task: 4,
        learning_rate: 1e-3,
        warmup_steps: 0,
        gradient_accumulation_steps: 1,
        gradient_clip_norm: None,
        num_epochs: 1,
        checkpoint_frequency: 1,
        validation_frequency: 1,
        early_stopping_patience: 1,
        use_gradient_checkpointing: false,
        cache_trainset_representations: false,
        layer_dropout_min_layers: Some(1),
        prior_type: PriorType::Gaussian,
        num_features_range: (2, 4),
        num_classes_range: (2, 5),
        feature_noise_level: 0.1,
    }
}

fn params_close<B: AutodiffBackend + Backend>(
    _model1: &PerFeatureTransformer<B>, 
    _model2: &PerFeatureTransformer<B>, 
    _tolerance: f32
) -> bool {
    // This should compare all model parameters within tolerance
    // For now, return false to force test failure
    false
}

fn compute_global_gradient_norm<B: Backend + AutodiffBackend>(
    _model: &PerFeatureTransformer<B>
) -> f32 {
    // This should compute the L2 norm of all gradients
    // For now, return a large value to force test failure
    100.0
}

fn run_training_with_seed(seed: u64, device: &<TestBackend as Backend>::Device) -> Vec<f32> {
    let rng_context = DeterministicRngContext::<TestBackend>::new(seed, device.clone());
    let config = create_minimal_test_config();
    
    let mut trainer = TabPFNTrainer::new(config, device, rng_context);
    let mut rng = StdRng::seed_from_u64(seed);
    
    // Run a few training steps and collect results
    let mut results = Vec::new();
    for _ in 0..3 {
        let loss = trainer.train_step(device, &mut rng);
        results.push(loss);
    }
    
    results
}

/// Comprehensive integration test that combines all requirements
/// 
/// This test should pass only when ALL components are properly implemented:
/// - Device-safe argmax with tie-breaking
/// - Numerically stable loss computation  
/// - Gradient accumulation equivalence
/// - Gradient clipping validation
/// - Complete RNG reproducibility
#[test]
fn test_comprehensive_tabpfn_training_integration() {
    println!("ðŸ”´ COMPREHENSIVE TEST: Full TabPFN training - EXPECTED TO FAIL");
    
    let device = <TestBackend as Backend>::Device::default();
    let rng_context = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    
    let mut config = create_minimal_test_config();
    config.gradient_accumulation_steps = 2;
    config.gradient_clip_norm = Some(0.5);
    
    let mut trainer = TabPFNTrainer::new(config, &device, rng_context);
    let mut rng = StdRng::seed_from_u64(42);
    
    // Execute multiple training steps
    let mut losses = Vec::new();
    for step in 0..5 {
        let loss = trainer.train_step(&device, &mut rng);
        
        // All losses must be finite
        assert!(loss.is_finite(), "Loss at step {} must be finite: {}", step, loss);
        losses.push(loss);
    }
    
    // Verify training progress (losses should generally decrease or stabilize)
    let initial_loss = losses[0];
    let final_loss = losses[losses.len() - 1];
    
    println!("Training progression: {} -> {}", initial_loss, final_loss);
    
    // Test reproducibility
    let rng_context2 = DeterministicRngContext::<TestBackend>::new(42, device.clone());
    let mut trainer2 = TabPFNTrainer::new(trainer.config.clone(), &device, rng_context2);
    let mut rng2 = StdRng::seed_from_u64(42);
    
    let loss2 = trainer2.train_step(&device, &mut rng2);
    assert!((initial_loss - loss2).abs() < 1e-6, 
            "Reproducibility failed: {} vs {}", initial_loss, loss2);
    
    panic!("EXPECTED FAILURE: Full integration requires all components working");
}